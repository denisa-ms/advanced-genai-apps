{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LLMs to generate synthetic data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, HTML, JSON, Markdown, Image\n",
    "import textwrap\n",
    "\n",
    "load_dotenv()\n",
    "AISTUDIO_AZURE_OPENAI_KEY = os.getenv(\"AISTUDIO_AZURE_OPENAI_KEY\")\n",
    "AISTUDIO_AZURE_OPENAI_ENDPOINT = os.getenv(\"AISTUDIO_AZURE_OPENAI_ENDPOINT\")\n",
    "AISTUDIO_OPENAI_GPT4_DEPLOYMENT_NAME = os.getenv(\"AISTUDIO_OPENAI_GPT4_DEPLOYMENT_NAME\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = AISTUDIO_AZURE_OPENAI_ENDPOINT, \n",
    "  api_key=AISTUDIO_AZURE_OPENAI_KEY,  \n",
    "  api_version=\"2024-02-01\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openAI(text):\n",
    "    system_message = \"\"\"\n",
    "    Please read the following CONTEXT and generate two question and answer json objects in an array based on the CONTEXT provided. \n",
    "    The questions should require deep reading comprehension, logical inference, deduction, and connecting ideas across the text. \n",
    "    Avoid simplistic retrieval or pattern matching questions. Instead, focus on questions that test the ability to reason about the text in complex ways, draw subtle conclusions, and combine multiple pieces of information to arrive at an answer. Ensure that the questions are relevant, specific, and cover the key points of the CONTEXT.  Provide concise answers to each question, directly quoting the text from provided context. Provide the array output in strict JSON format as shown in output format. Ensure that the generated JSON is 100 percent structurally correct, with proper nesting, comma placement, and quotation marks. There should not be any comma after last element in the array.\n",
    "  \n",
    "    Output format:\n",
    "    [\n",
    "    {\n",
    "        \"question\": \"Question 1\",\n",
    "        \"answer\": \"Answer 1\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Question 2\",\n",
    "        \"answer\": \"Answer 2\"\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    CONTEXT:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=AISTUDIO_OPENAI_GPT4_DEPLOYMENT_NAME,\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":system_message},\n",
    "            {\"role\":\"user\",\"content\":text}\n",
    "            ],\n",
    "        temperature=1\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def prettyprint(text: str) -> str:\n",
    "    print(textwrap.fill(text, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[\n",
       "    {\n",
       "        \"question\": \"Why does CoH add a regularization term to the model during pre-training?\",\n",
       "        \"answer\": \"To avoid overfitting and to maximize the log-likelihood of the pre-training dataset.\"\n",
       "    },\n",
       "    {\n",
       "        \"question\": \"What strategies has the agent learned to employ to address its limitations in handling dynamic or proprietary information?\",\n",
       "        \"answer\": \"The agent learns to call external APIs for extra information that is missing from the model weights, including current information, code execution capability, access to proprietary information sources and more.\"\n",
       "    }\n",
       "]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens,\n",
    "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\n",
    "\"\"\"\n",
    "answer = call_openAI(context)\n",
    "display(HTML(answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
