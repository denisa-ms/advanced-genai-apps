{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with prompty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Learning Objectives** - Upon completing this tutorial, you should be able to:\n",
    "\n",
    "- Write LLM application using prompty and visualize the trace of your application.\n",
    "- batch run prompty against multi lines of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Execute a Prompty\n",
    "\n",
    "Prompty is a file with .prompty extension for developing prompt template. \n",
    "The prompty asset is a markdown file with a modified front matter. \n",
    "The front matter is in yaml format that contains a number of metadata fields which defines model configuration and expected inputs of the prompty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"basic.prompty\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: before running below cell, please configure required environment variable `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT` by create an `.env` file. Please refer to `../.env.example` as an template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "#create a .env file with the following variables and replace with your values\n",
    "AISTUDIO_AZURE_OPENAI_KEY = os.getenv(\"AISTUDIO_AZURE_OPENAI_KEY\")\n",
    "AISTUDIO_AZURE_OPENAI_ENDPOINT = os.getenv(\"AISTUDIO_AZURE_OPENAI_ENDPOINT\")\n",
    "AISTUDIO_OPENAI_GPT4_DEPLOYMENT_NAME = os.getenv(\"AISTUDIO_OPENAI_GPT4_DEPLOYMENT_NAME\")\n",
    "AZURE_SUBSCRIPTION_ID = os.getenv(\"AZURE_SUBSCRIPTION_ID\")\n",
    "AZURE_AISTUDIO_PROJECT_RESOURCE_GROUP = os.getenv(\"AZURE_AISTUDIO_PROJECT_RESOURCE_GROUP\")\n",
    "AZURE_AISTUDIO_PROJECT_NAME = os.getenv(\"AZURE_AISTUDIO_PROJECT_NAME\")\n",
    "OPENAI_API_VERSION = os.getenv(\"OPENAI_API_VERSION\")\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: before running below cell, please configure required environment variable AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT by create an .env file. Please refer to ../.env.example as an template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from promptflow.core import Prompty\n",
    "\n",
    "# load prompty as a flow\n",
    "f = Prompty.load(source=\"basic.prompty\")\n",
    "\n",
    "# execute the flow as function\n",
    "result = f(question=\"What is the capital of France?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can override configuration with `AzureOpenAIModelConfiguration` and `OpenAIModelConfiguration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from promptflow.core import AzureOpenAIModelConfiguration, OpenAIModelConfiguration\n",
    "\n",
    "# override configuration with AzureOpenAIModelConfiguration - in this case I am using the same model just to show how to use it\n",
    "configuration = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=AISTUDIO_AZURE_OPENAI_ENDPOINT,  # Use ${env:<ENV_NAME>} to surround the environment variable name.\n",
    "    api_key=AISTUDIO_AZURE_OPENAI_KEY,\n",
    "    azure_deployment=AISTUDIO_OPENAI_GPT4_DEPLOYMENT_NAME,\n",
    ")\n",
    "\n",
    "\n",
    "override_model = {\"configuration\": configuration, \"parameters\": {\"max_tokens\": 512}}\n",
    "\n",
    "# load prompty as a flow\n",
    "f = Prompty.load(source=\"basic.prompty\", model=override_model)\n",
    "\n",
    "# execute the flow as function\n",
    "result = f(question=\"What is the capital of France?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize trace by using start_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n"
     ]
    }
   ],
   "source": [
    "from promptflow.tracing import start_trace\n",
    "\n",
    "# start a trace session, and print a url for user to check trace\n",
    "start_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run below cell will collect a trace in trace UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23334/v1.0/ui/traces/?#collection=5-E2E&uiTraceId=0x7b96ce84ff520b9d2d14bdbdfe0731c3\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23334/v1.0/ui/traces/?#collection=5-E2E&uiTraceId=0xb1ca0a2bef6af9ec05a7d22679171776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'llm.usage.completion_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of Japan is Tokyo.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rerun the function, which will be recorded in the trace\n",
    "question = \"What is the capital of Japan?\"\n",
    "ground_truth = \"Tokyo\"\n",
    "result = f(question=question)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval the result \n",
    "\n",
    "Note: the eval flow returns a `json_object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'llm.usage.completion_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': '5',\n",
       " 'explanation': 'The answer correctly identifies Tokyo as the capital of Japan.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load prompty as a flow\n",
    "eval_flow = Prompty.load(\"eval.prompty\")\n",
    "# execute the flow as function\n",
    "result = eval_flow(question=question, ground_truth=ground_truth, answer=result)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch run with multi-line data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.client import PFClient\n",
    "\n",
    "pf = PFClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-22 12:18:17 +0300][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run n5_e2e_20240922_121817_263398, log path: C:\\Users\\dschlesinger\\.promptflow\\.runs\\n5_e2e_20240922_121817_263398\\logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23334/v1.0/ui/traces/?#run=n5_e2e_20240922_121817_263398\n",
      "2024-09-22 12:18:17 +0300   50932 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2024-09-22 12:18:18 +0300   50932 execution.bulk     INFO     Current system's available memory is 19803.12890625MB, memory consumption of current process is 186.96875MB, estimated available worker count is 19803.12890625/186.96875 = 105\n",
      "2024-09-22 12:18:18 +0300   50932 execution.bulk     INFO     Set process count to 3 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 3, 'estimated_worker_count_based_on_memory_usage': 105}.\n",
      "2024-09-22 12:18:21 +0300   50932 execution.bulk     INFO     Process name(SpawnProcess-3)-Process id(5092)-Line number(0) start execution.\n",
      "2024-09-22 12:18:21 +0300   50932 execution.bulk     INFO     Process name(SpawnProcess-2)-Process id(29320)-Line number(1) start execution.\n",
      "2024-09-22 12:18:21 +0300   50932 execution.bulk     INFO     Process name(SpawnProcess-4)-Process id(15180)-Line number(2) start execution.\n",
      "2024-09-22 12:18:23 +0300   50932 execution.bulk     INFO     Process name(SpawnProcess-2)-Process id(29320)-Line number(1) completed.\n",
      "2024-09-22 12:18:24 +0300   50932 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2024-09-22 12:18:24 +0300   50932 execution.bulk     INFO     Average execution time for completed lines: 3.03 seconds. Estimated time for incomplete lines: 6.06 seconds.\n",
      "2024-09-22 12:18:26 +0300   50932 execution.bulk     INFO     Process name(SpawnProcess-3)-Process id(5092)-Line number(0) completed.\n",
      "2024-09-22 12:18:26 +0300   50932 execution.bulk     INFO     Process name(SpawnProcess-4)-Process id(15180)-Line number(2) completed.\n",
      "2024-09-22 12:18:26 +0300   50932 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2024-09-22 12:18:26 +0300   50932 execution.bulk     INFO     Average execution time for completed lines: 1.68 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2024-09-22 12:18:26 +0300   50932 execution.bulk     INFO     The thread monitoring the process [29320-SpawnProcess-2] will be terminated.\n",
      "2024-09-22 12:18:26 +0300   50932 execution.bulk     INFO     The thread monitoring the process [5092-SpawnProcess-3] will be terminated.\n",
      "2024-09-22 12:18:26 +0300   50932 execution.bulk     INFO     The thread monitoring the process [15180-SpawnProcess-4] will be terminated.\n",
      "2024-09-22 12:18:26 +0300    5092 execution.bulk     INFO     The process [5092] has received a terminate signal.\n",
      "2024-09-22 12:18:26 +0300   29320 execution.bulk     INFO     The process [29320] has received a terminate signal.\n",
      "2024-09-22 12:18:26 +0300   15180 execution.bulk     INFO     The process [15180] has received a terminate signal.\n",
      "2024-09-22 12:18:27 +0300   50932 execution.bulk     INFO     Process 29320 terminated.\n",
      "2024-09-22 12:18:27 +0300   50932 execution.bulk     INFO     Process 5092 terminated.\n",
      "2024-09-22 12:18:27 +0300   50932 execution.bulk     INFO     Process 15180 terminated.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"n5_e2e_20240922_121817_263398\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2024-09-22 12:18:17.263398+03:00\"\n",
      "Duration: \"0:00:10.254944\"\n",
      "Output path: \"C:\\Users\\dschlesinger\\.promptflow\\.runs\\n5_e2e_20240922_121817_263398\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flow = \"./basic.prompty\"  # path to the prompty file\n",
    "data = \"./data.jsonl\"  # path to the data file\n",
    "\n",
    "# create run with the flow and data\n",
    "base_run = pf.run(\n",
    "    flow=flow,\n",
    "    data=data,\n",
    "    column_mapping={\n",
    "        \"question\": \"${data.question}\",\n",
    "    },\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>inputs.line_number</th>\n",
       "      <th>outputs.output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is capital of France?</td>\n",
       "      <td>0</td>\n",
       "      <td>The capital of France is Paris.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the meaning of life?</td>\n",
       "      <td>1</td>\n",
       "      <td>The meaning of life is a philosophical questio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the planets in Sun system?</td>\n",
       "      <td>2</td>\n",
       "      <td>The planets in the Solar System are Mercury, V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       inputs.question  inputs.line_number  \\\n",
       "0           What is capital of France?                   0   \n",
       "1         What is the meaning of life?                   1   \n",
       "2  What are the planets in Sun system?                   2   \n",
       "\n",
       "                                      outputs.output  \n",
       "0                    The capital of France is Paris.  \n",
       "1  The meaning of life is a philosophical questio...  \n",
       "2  The planets in the Solar System are Mercury, V...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details = pf.get_details(base_run)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate your flow\n",
    "Then you can use an evaluation method to evaluate your flow. The evaluation methods are also flows which usually using LLM assert the produced output matches certain expectation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation on the previous batch run\n",
    "The **base_run** is the batch run we completed in step 2 above, for web-classification flow with \"data.jsonl\" as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-22 12:14:50 +0300][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run basic_20240922_121450_688435, log path: C:\\Users\\dschlesinger\\.promptflow\\.runs\\basic_20240922_121450_688435\\logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23334/v1.0/ui/traces/?#run=basic_20240922_121450_688435\n",
      "2024-09-22 12:14:50 +0300    7516 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2024-09-22 12:14:50 +0300    7516 execution.bulk     INFO     Current system's available memory is 18091.234375MB, memory consumption of current process is 211.125MB, estimated available worker count is 18091.234375/211.125 = 85\n",
      "2024-09-22 12:14:50 +0300    7516 execution.bulk     INFO     Set process count to 3 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 3, 'estimated_worker_count_based_on_memory_usage': 85}.\n",
      "2024-09-22 12:14:54 +0300    7516 execution.bulk     INFO     Process name(SpawnProcess-10)-Process id(21472)-Line number(0) start execution.\n",
      "2024-09-22 12:14:54 +0300    7516 execution.bulk     INFO     Process name(SpawnProcess-12)-Process id(50528)-Line number(1) start execution.\n",
      "2024-09-22 12:14:54 +0300    7516 execution.bulk     INFO     Process name(SpawnProcess-11)-Process id(18368)-Line number(2) start execution.\n",
      "2024-09-22 12:14:55 +0300    7516 execution.bulk     INFO     Process name(SpawnProcess-10)-Process id(21472)-Line number(0) completed.\n",
      "2024-09-22 12:14:56 +0300    7516 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2024-09-22 12:14:56 +0300    7516 execution.bulk     INFO     Average execution time for completed lines: 2.03 seconds. Estimated time for incomplete lines: 4.06 seconds.\n",
      "2024-09-22 12:14:58 +0300    7516 execution.bulk     INFO     Process name(SpawnProcess-11)-Process id(18368)-Line number(2) completed.\n",
      "2024-09-22 12:14:59 +0300    7516 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2024-09-22 12:14:59 +0300    7516 execution.bulk     INFO     Average execution time for completed lines: 2.52 seconds. Estimated time for incomplete lines: 2.52 seconds.\n",
      "2024-09-22 12:14:59 +0300    7516 execution.bulk     INFO     Process name(SpawnProcess-12)-Process id(50528)-Line number(1) completed.\n",
      "2024-09-22 12:15:00 +0300    7516 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2024-09-22 12:15:00 +0300    7516 execution.bulk     INFO     Average execution time for completed lines: 2.02 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2024-09-22 12:15:00 +0300    7516 execution.bulk     INFO     The thread monitoring the process [50528-SpawnProcess-12] will be terminated.\n",
      "2024-09-22 12:15:00 +0300    7516 execution.bulk     INFO     The thread monitoring the process [21472-SpawnProcess-10] will be terminated.\n",
      "2024-09-22 12:15:00 +0300    7516 execution.bulk     INFO     The thread monitoring the process [18368-SpawnProcess-11] will be terminated.\n",
      "2024-09-22 12:15:00 +0300   50528 execution.bulk     INFO     The process [50528] has received a terminate signal.\n",
      "2024-09-22 12:15:00 +0300   21472 execution.bulk     INFO     The process [21472] has received a terminate signal.\n",
      "2024-09-22 12:15:00 +0300   18368 execution.bulk     INFO     The process [18368] has received a terminate signal.\n",
      "2024-09-22 12:15:00 +0300    7516 execution.bulk     INFO     Process 21472 terminated.\n",
      "2024-09-22 12:15:00 +0300    7516 execution.bulk     INFO     Process 18368 terminated.\n",
      "2024-09-22 12:15:00 +0300    7516 execution.bulk     INFO     Process 50528 terminated.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"basic_20240922_121450_688435\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2024-09-22 12:14:50.688435+03:00\"\n",
      "Duration: \"0:00:10.282174\"\n",
      "Output path: \"C:\\Users\\dschlesinger\\.promptflow\\.runs\\basic_20240922_121450_688435\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_prompty = \"./eval.prompty\"\n",
    "\n",
    "eval_run = pf.run(\n",
    "    flow=eval_prompty,\n",
    "    data=\"./data.jsonl\",  # path to the data file\n",
    "    run=base_run,  # specify base_run as the run you want to evaluate\n",
    "    column_mapping={\n",
    "        \"question\": \"${data.question}\",\n",
    "        \"answer\": \"${run.outputs.output}\",  \n",
    "        \"ground_truth\": \"${data.ground_truth}\",\n",
    "    },\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>inputs.answer</th>\n",
       "      <th>inputs.ground_truth</th>\n",
       "      <th>inputs.line_number</th>\n",
       "      <th>outputs.score</th>\n",
       "      <th>outputs.explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is capital of France?</td>\n",
       "      <td>The capital of France is Paris.</td>\n",
       "      <td>Paris</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>The answer correctly identifies Paris as the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the meaning of life?</td>\n",
       "      <td>The meaning of life is a philosophical questio...</td>\n",
       "      <td>The meaning of life is subjective and can vary...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>The answer comprehensively addresses the philo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the planets in Sun system?</td>\n",
       "      <td>The planets in the Solar System are Mercury, V...</td>\n",
       "      <td>The planets in the Solar System are Mercury, V...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>The answer correctly lists all the planets in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       inputs.question  \\\n",
       "0           What is capital of France?   \n",
       "1         What is the meaning of life?   \n",
       "2  What are the planets in Sun system?   \n",
       "\n",
       "                                       inputs.answer  \\\n",
       "0                    The capital of France is Paris.   \n",
       "1  The meaning of life is a philosophical questio...   \n",
       "2  The planets in the Solar System are Mercury, V...   \n",
       "\n",
       "                                 inputs.ground_truth  inputs.line_number  \\\n",
       "0                                              Paris                   0   \n",
       "1  The meaning of life is subjective and can vary...                   1   \n",
       "2  The planets in the Solar System are Mercury, V...                   2   \n",
       "\n",
       "   outputs.score                                outputs.explanation  \n",
       "0              5  The answer correctly identifies Paris as the c...  \n",
       "1              5  The answer comprehensively addresses the philo...  \n",
       "2              5  The answer correctly lists all the planets in ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details = pf.get_details(eval_run)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize run using ui\n",
    "pf.visualize([base_run, eval_run])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "By now you've successfully run your first prompt flow and even did evaluation on it. That's great!\n",
    "\n",
    "You can check out more examples:\n",
    "- [Basic Chat](https://github.com/microsoft/promptflow/tree/main/examples/prompty/chat-basic): demonstrates how to create a chatbot that can remember previous interactions and use the conversation history to generate next message."
   ]
  }
 ],
 "metadata": {
  "build_doc": {
   "author": [
    "lalala123123@github.com",
    "wangchao1230@github.com"
   ],
   "category": "local",
   "section": "Prompty",
   "weight": 10
  },
  "description": "A quickstart tutorial to run a prompty and evaluate it.",
  "kernelspec": {
   "display_name": "prompt_flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "resources": "examples/requirements.txt, examples/prompty/basic, examples/prompty/eval-basic"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
